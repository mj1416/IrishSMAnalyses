\lhead{\emph{Forecasting Electric Load}}  % Set the left side page header to "Abbreviations"

\chapter{Electric Load Forecasts}
We reviewed many of the existing forecasts in section \ref{subsec:litrev} and we discussed the value of having good forecasts with some comments on how the quality of a forecast may be judged. In this chapter, we will generate a few of these forecast and discuss their goodness both qualitatively and quantitatively. Due to the strong periodic and symmetric correlation (fig. \ref{fig:acf_day}), we can confidently use many of the common forecasting methods in the literature and build on those. Recall that we have 7 weeks of data and we treat weeks 16-21 (inclusive) as past data and forecast the load in week 22.

\section{Forecasts} \label{subsec:forecasts}

\subsection{Similar Day Forecast}

The first forecast considered is the Similar Day (SD) forecast which forms the foundation of most electric load forecasting methods and is quite successful in itself as we'll see later. Recall that our first objective is to establish a benchmark forecast. Because of its prevalence, the SD forecast is a good candidate to be this benchmark. The SD forecast is generated as follows:
\begin{enumerate}
\item The forecast at a given half hour for a specific day of the week is taken to be the arithmetic mean of all past observations which exist at the same half hour for the same day of the week. For example, the forecast for 9 am on Monday of week 22 will be the arithmetic mean of measurements taken at 9 am on Monday of weeks 16 to 21.
\item This is repeated for each half hour for each day of the week for each household.
\item Thus a week long forecast is generated at half hourly resolution.
\end{enumerate}

A sample forecast showing also the measured load for one customer is provided in figure \ref{fig:SDforecast}. The numbering of customers has been randomised to ensure confidentiality so while the choice is random, this household portrays some of the key advantages and disadvantages of each forecast. From figure \ref{fig:SDforecast}, we can see that though the peaks are largely underestimated, they occur at roughly the right time, specifically the breakfast hours for the weekdays. Similarly, we see peaks in the forecast for the evening time are also underestimated but moreover the forecast is smoother than the observed load. This is because the breakfast peak occurs over a narrower time window than the evening peak and doesn't get as smoothed due to the week to week variability. Additionally, it maybe argued that the underestimation of peaks is an artefact of reduced load in the first few weeks of the past data (recall fig. \ref{fig:sums}).

\begin{figure}
\centering
\includegraphics[scale=0.5]{forecast_SD_P1.pdf}
\caption{SD forecast and the observations for customer 1.}
\label{fig:SDforecast} 
\end{figure}

\subsection{Last Week Forecast}

Another common but simple forecasting technique is the Last Week (LW) forecast. It can be seen as a special case of the SD forecast where instead of using all historical data only the most recent week is used. Even before we test this method, it is probable that this is not a good forecasting method in general as relevant data, if available, may go unused and does not take into account the week to week variability. However, where data is not available, this may be the only option and in some cases may be a good enough option i.e for households which are very predictable.

As before, for the same customer, the LW forecast as well as the observations are shown in figure \ref{fig:LW_forecast_P1}. At least in the sample case the LW forecast is not very well suited to representing the peaks accurately and is particularly vulnerable due to the week to week volatility in load. However, it can be seen that there are cases where, even though the magnitude of the peak is not correct, the peaks are being forecasted at roughly the right time. With this in mind we can combine the ideas from the SD and LW forecasts to generate an adjusted average forecast.

\begin{figure}
\includegraphics[scale=0.5]{forecast_LW_P1.pdf}
\caption{LW forecast and obseervation for customer 1.}
\label{fig:LW_forecast_P1} 
\end{figure}

\subsection{Adjusted Average Forecast}

\cite{dan14} introduced an adjusted average (AA) forecast. The idea is that we start with a base profile and iteratively update it to included information from previous weeks with the most recent week having the most influence on the end forecast. The iterative update involves a local, restricted permutation in time which minimises some cost function. The forecast, as described by \cite{dan14}, forecasts each day of the week individually though analogously and the algorithm is as follows:
\begin{enumerate}[label=\roman*)]
\item Let $d$ be the day of the week being forecasted $(d=1,...,7)$ and suppose that there are $N$ daily usage profiles (i.e. historical data for day $d$ exists for the most recent $N$ weeks) at half hourly resolution. For $k = 1, ..., N$, the daily usage profile is denoted by $\boldsymbol{G}^{(k)} = (g_1^{(k)}, ... , g_{48}^{(k)})^T$. Note the convention that $\boldsymbol{G}^{(1)}$ is the profile for day $d$ in the most recent week whereas $\boldsymbol{G}^{(N)}$ is of the earliest week.
\item A base profile, $\boldsymbol{F}^{(1)} = \left(f_1^{(1)}, ... , f_{48}^{(1)} \right)^T$, where each $f_i^{(1)}$ is defined by the median of past load of the corresponding half hour i.e. $ \forall i \in (1, ..., 48), \quad f_i^{(1)} = \text{median}(g_i^{(1)}, ..., g_i^{(N)})$.
\item This baseline is updated iteratively to get the final forecast, $\boldsymbol{F}^{(N)}$, in the following way. Suppose, at iteration $k$, we have $\boldsymbol{F}^{(k)}$ for $1 \le k \le N-1$, then $\boldsymbol{F}^{(k+1)}$ is obtained by setting $\boldsymbol{F}^{(k+1)} = \frac{1}{k+1} \left( \boldsymbol{\hat{G}}^{(k)} + k \boldsymbol{F}^{(k)}\right)$, where $\boldsymbol{\hat{G}}^{(k)} = \hat{P}\boldsymbol{G}^{(k)}$ with $\hat{P} \in  \mathscr{P}$ being a permutation matrix s.t. $||\hat{P}\boldsymbol{G}^{(k)} - \boldsymbol{F}^{(k)}||_4 = \displaystyle \min_{P \in \mathscr{P}}||P\boldsymbol{G}^{(k)} - \boldsymbol{F}^{(k)}||_4 $.
\item $\mathscr{P}$ is the set of restricted permutations i.e, for a chosen deformation limit $\omega$, the load at half hour $i$ can be associated to the load at half hour $j$ if $|i-j| \le \omega$. In \cite{dan14} and this report, $\omega=3$.
\item Then, the final forecast is given by $\boldsymbol{F}^{(N)} = \frac{1}{N+1}\left(\displaystyle \sum_{k=1}^N \boldsymbol{\hat{G}}^{(k)} + \boldsymbol{F}^{(1)} \right)$.
\end{enumerate}

The Hungarian algorithm is used to solve the minimisation problem above, which can be thought of as an optimisation problem. The Hungarian algorithm uses a cost matrix and thus for this implementation we must specify this matrix. Recall that we have a deformation limit, $\omega$ which we set to 3. This comes into play as for any index $i = 1, ..., 48$, which relates to half hour, the $(i,j)^{th}$ element is set to be the cost if $|i-j|\le\omega$ or set to be infinity if $|i-j| > \omega$. Thus at iteration $k$, the cost matrix looks like: \newline

\begin{tiny}\centerline{$\begin{bmatrix}
    |g_1^{(k)} - f_1^{(k)}| & |g_2^{(k)} - f_1^{(k)}| & |g_3^{(k)} - f_1^{(k)}| & |g_4^{(k)} -f_1^{(k)}| & \infty  & \infty& \infty & \infty & \dots & \infty \\
    |g_1^{(k)} - f_2^{(k)}| & |g_2^{(k)} - f_2^{(k)}| & |g_3^{(k)} - f_2^{(k)}| &  |g_4^{(k)} - f_2^{(k)}| & |g_5^{(k)} - f_2^{(k)}| & \infty & \infty &\infty & \dots & \infty\\
    |g_1^{(k)} - f_3^{(k)}| & |g_2^{(k)} - f_3^{(k)}| & |g_3^{(k)} - f_3^{(k)}| &  |g_4^{(k)} - f_3^{(k)}|& |g_5^{(k)}- f_3^{(k)}| & |g_6^{(k)}-f_3^{(k)}| & \infty & \infty & \dots  & \infty\\
    |g_1^{(k)}-f_4^{(k)}| & |g_2^{(k)} - f_4^{(k)}| & |g_3^{(k)} - f_4^{(k)}| & |g_4^{(k)} - f_4^{(k)}| &  |g_5^{(k)} - f_4^{(k)}| & |g_6^{(k)}- f_4^{(k)}|& |g_7^{(k)} - f_4^{(k)}| &\infty &\dots & \infty\\
    \infty & |g_2^{(k)} - f_5^{(k)}| & |g_3^{(k)} - f_5^{(k)}| & |g_4^{(k)} - f_5^{(k)}| &  |g_5^{(k)} - f_5^{(k)}|& |g_6^{(k)}- f_5^{(k)}|& |g_7^{(k)} - f_5^{(k)}| &|g_8^{(k)} - f_5^{(k)}| &\dots & \infty\\
     &  & \dots &  & &  & &  & \\
\end{bmatrix}$} \end{tiny}

This algorithm is quite suited for forecasting as it gives a peaky forecast and at least for the sample forecast (fig. \ref{fig:AA_forecast_P1}) we can see that this is indeed the case. Obviously not all peaks are represented well however from looking at this one customer, it seems that more peaks are being forecasted and roughly the correct times. The magnitudes are still not correct however in many cases they are closer to the observed demand than in the SD forecast. We will quantify this further in section \ref{subsec:errs}. This should be the best forecast at least when the error measure used is the adjusted error measure.

\begin{figure}
\centering
\includegraphics[scale=0.5]{forecast_AA_P1.pdf}
\caption{AA forecast and observation for customer 1.}
\label{fig:AA_forecast_P1} 
\end{figure}

\subsection{Weighted Average Forecast}

The SD forecast weighted all past data equally and didn't use any window as in the AA forecast. Instead of guessing the weights we can calculate the weights using linear regression. Since these forecasts may be used by non-mathematicians it is valuable to reinforce the appropriateness of the linear regression technique. Linear regression is appropriate in applications where the model depends linearly on the unknown parameters, $\boldsymbol \beta$, (equation \ref{eq:lin_reg}) but the relationship between the features need not be linear.

Given a data set $\{y_i, x_{i1}, ... , x_{ip}\}_{i=1}^n$, linear regression, in vector form, can be represented as shown in equation \ref{eq:lin_reg}.

\begin{equation} \label{eq:lin_reg}
\textbf{y} = X \boldsymbol \beta +\boldsymbol \epsilon
\end{equation}
where $\textbf{y} = (y_1, y_2, ... , y_n)^T $, $X =  (\textbf{x}_1^T, \textbf{x}_2^T,, ... ,\textbf{x}_n^T)^T$,  $\textbf{x}_i^T, = (x_{i1}, ... , x_{ip})$ for $i = 1, ... , n$, $\boldsymbol \beta = (\beta_1 , ... , \beta_p)$ and $\boldsymbol \epsilon = (\epsilon_1, ... , \epsilon_n)$ with $( \cdot )^T$ denoting transpose. The $\boldsymbol \epsilon$ is a disturbance or error vector which adds noise to the model and is commonly assumed to have identically and independently distributed multivariate normal components each with zero mean and variance $\sigma^2$.

Thus, in the context of electric load, we can use linear regression to predict load at each half hour by modelling the relationship between the half hour in question and available observations that we deem relevant, where we inform ourselves of what is relevant by utilising the correlations (recall fig. \ref{fig:acf_day}). Thus by running 48 linear regressions, a one day forecast can be generated. The specifics of the implementation are as follows:
\begin{enumerate}
\item Linear regression is done on variable $X$ and $Y$, where $X$ is the matrix of features and $Y$ is the target variable.
%\item As mentioned before, the data set contains 7 weeks of data. These weeks range from 16 to 22. Week 22 is the week being forecasted and weeks 16 to 21 are considered "past" data which will form part of the feature matrix. 
\item The implementation contains two steps: \begin{enumerate} \item The first step is to find/learn the parameters $\boldsymbol \beta$. To do this both $X$ and $Y$ are required. In this first step, $Y$ is a vector which contains information about the half hour in question for each household for the most recent week i.e week 21 whereas matrix $X$ contains data for the same half hour as well a lag of 1 hour in each direction from weeks 16 to 20 (inclusive). Thus by using ordinary least squares, $\boldsymbol \beta$ is found by fitting $X$ to $Y$.
\item In the second step, the forecast is created. The implementation requires the feature matrix in the fitting phase and the forecasting phase to have the same dimension thus $X$ has to be adjusted. Since we are now forecasting week 22, we use the same half hour and lags but taken from weeks 17 to 21 to ensure that the new $X$ has the same dimensions as the old one. The values predicted by the model are considered to be the forecast for the half hour in question for week 22. \end{enumerate}
\item The forecast that is produced using the above procedure is for a prescribed half hour of a prescribed day of the week. Thus, both the desired half hour and the day of the week are required inputs of the implementation.
\item The daily forecast for day $d$ is then produced by repeating the two steps for each of the 48 half hours.
\item Then acquiring the weekly forecast means running the daily forecast for each day of the week.
\end{enumerate}
%Using the \texttt{Python's SciKit} package, 

In this way it is possible to think of the SD forecast as a special case of the linear regression where the coefficients/parameters of the lag are zero in the SD forecast and the others are equal to 1 regardless of when measurements were taken. This forecast will be referred to as the Weighted Average (WA) forecast, a sample of which along with the observations is shown in figure \ref{fig:LR_forecast_P1}. Given that many of the existing algorithms in the literature are based around regression techniques, it is reasonable to expect that this is better than the LW forecast and also potentially the SD forecast given that it uses more data and that the weights have been calculated. However, since the load profile is irregular, the peaks are likely to get smoothed in the process and may not be suitable for STLF at LV level. This can explicitly be seen in the sample forecast given in figure \ref{fig:LR_forecast_P1} but we will quantify this more concretely as we proceed. %and \ref{fig:forecasts_P1}. 
\begin{figure}
\centering
\includegraphics[scale=0.5]{forecast_LR_P1.pdf}
\caption{Weighted Average forecast and observation for customer 1.}
\label{fig:LR_forecast_P1} 
\end{figure}

\subsection{Bayesian Ridge Regression Forecast} % \label{subsubsec:BR}

Since we have an example of at least one paper \citep{douglas98} where the Bayesian framework has been applied, we can apply it too. \texttt{Python's SciKit} package provides an in built option with ridge regression, thus we call the following algorithm Bayesian Ridge Regression (BRR) forecast. Before looking at the results, we review the basic background of ridge regression. Ridge regression is the most common method of regularisation of ill-posed problems in statistics. A problem is said to be ill-posed if it has no solution or multiple solutions. Suppose we wish to find an $\boldsymbol{x}$ such that $A\boldsymbol{x}=\boldsymbol{b}$, where $A$ is a matrix and $\boldsymbol{x}$ and $\boldsymbol{b}$ are vectors. The ordinary least squares estimation solution would be a gained by a minimisation of $||A\boldsymbol{x} - \boldsymbol{b}||_2$ however for the ill-posed problem, this solution may be over-fitted or under-fitted. To give preference to a solution with desirable properties, the relgularisation term $||\Gamma\boldsymbol{x}||_2$ is added so that the minimisation is of $||A\boldsymbol{x} - \boldsymbol{b}||_2 + ||\Gamma\boldsymbol{x}||_2$, which gives the solution $\hat{\boldsymbol{x}} = \left(A^T A + \Gamma^T \Gamma \right)^{-1} A^T \boldsymbol{b}$. For the Bayesian interpretation, simplistically this regularised solution is the most probable solution given the data and the prior distribution for $\boldsymbol{x}$ according to Bayes' Theorem. The implementation is very much like the weighted average, where the coefficients are identified using the weeks 16 to 20 and the forecast for week 22 is generated using weeks 17 to 21.

Lastly, we visualise a sample of BRR forecast along with the observations for one customer (fig. \ref{fig:BR_forecast_P1}). The forecast, at least for this customer, is qualitatively very similar to the WA forecast as we would expect and smooth. This may make this forecast unsuitable for the context of load forecasts as before. It is also noteworthy that though this data set has no bank holidays and change of seasons, it does have a fundamental shift in behaviour due to schools opening half way through the data. Perhaps these algorithms would be more successful if the forecasted week were also very similar to all the historical data being used to generate the forecast.

\begin{figure}
\centering
\includegraphics[scale=0.5]{forecast_BR_P1.pdf}
\caption{Bayesian Ridge Regression forecast and observation for customer 1.}
\label{fig:BR_forecast_P1} 
\end{figure}


Recall that we started this section with the goal of setting a benchmark forecast. We reviewed 4 more algorithms in addition to the SD forecast but before we decide which of these is to serve as a benchmark, we quantitatively validate each one.

%\clearpage
\section{Forecast Validation} \label{subsec:errs}
As was discussed before in section \ref{subsec:litrev}, there are many ways to judge a forecast. There is no absolute way of measuring the goodness of a forecast and depending on the error measure used, the conclusion may even be qualitatively different (as will be seen later). Though there are no correct measures, some measures may be more appropriate or suitable than others depending on the application. In this section we will consider two error measures, one which is quite commonly used and another which was developed by \cite{dan14} with electric load in mind.

The first one is the common $p-$norm error described in equation \ref{eq:err_p}, where $ p > 1 $.

\begin{equation}\label{eq:err_p}
E_p \equiv ||\boldsymbol{f} - \boldsymbol{a}||_p := \left( \sum_{i=1}^{n} |f_i - a_i |^p\right)^{\frac{1}{p}},
\end{equation}

where $\boldsymbol{f}$ is the forecast that is $n$ half hours long and $\boldsymbol{a}$ is the observed measurement. This gives the $p-$norm error for each household. Commonly, $p$ is taken to be 2, which translates to root mean square error however in this report we will take $p=4$ as has been done by \citet{dan14} so as to exaggerate larger errors. This was calculated for each household and is shown in figure \ref{fig:m4e_all}. The black line is the error at each household whereas the flat blue line is the average of all the errors. These average has been presented in table \ref{tab:errs}.

\begin{figure}
\centering
\includegraphics[scale=0.5]{Errors_E_4.pdf}
\caption{\label{fig:m4e_all} $4^{th}$-norm error of each household (black) and their average (blue).}
\end{figure}


The $4^{\textbf{th}}-$norm error, $E_4$, provides a good benchmark for an error measure, however as was discussed before a point error metric such as this one falls prey to the double penalty effect. In order to reduce this effect we can do a restricted permutation locally in time by using the error measure introduced in \citet{dan14}. The mathematical set up for this error measure is as follows:
\begin{itemize}
\item $\hat{E}_p^\omega = \displaystyle{\min_{P \in \mathscr{P}}||P\boldsymbol{f}-\boldsymbol{x}||_p}$, where $\boldsymbol{f}$ is the forecast and $\boldsymbol{x}$ are the observations and where $\mathscr{P}$ represents the set of restricted permutations i.e. we allow the forecast to be matched to an observation within some window. The window was chosen to be 2 and a half hours, i.e. $\omega = 3$, as in \cite{dan14}. It should be noted that the value of $\omega$ need not be the same for the forecast and the error measure.
\item The solution to above minimisation problem can be found using the Hungarian algorithm as for the AA forecast. The cost matrix for the error measure is given by


\begin{small}\centerline{$\begin{bmatrix}
    |f_1 - x_1| & |f_2 - x_1| & |f_3 - x_1| & |f_4 - x_1|  & \infty& \infty& \infty &\infty & \dots & \infty \\
    |f_1 - x_2| & |f_2 - x_2| & |f_3 - x_2| &  |f_4 - x_2| & |f_5 - x_2| & \infty  &\infty & \infty & \dots & \infty\\
    |f_1 - x_3| & |f_2 - x_3| & |f_3 - x_3| &  |f_4 - x_3|& |f_5- x_3|& |f_6 - x_3| & \infty&\infty & \dots  & \infty\\
    |f_1 - x_4 & |f_2 - x_4| & |f_3 - x_4| & |f_4 - x_4| &  |f_5 - x_4|& |f_6- x_4|& |f_7 - x_4| & \infty&\dots & \infty\\
    \infty & |f_2 - x_5| & |f_3 - x_5| & |f_4 - x_5| &  |f_5 - x_5|& |f_6- x_5|& |f_7 - x_5| & |f_8 -x_5|&\dots & \infty\\
   &&&& \dots &&&&& \\
\end{bmatrix}$} \end{small}

\noindent where we have chosen these values because the cost is the absolute difference between the forecast and observations.  
\item In this implementation, the above minimisation gives the adjusted error for each half hour for each household.
\end{itemize}

As for the $4^{\text{th}}-$norm error we can visualise the adjusted error but in order to do this we must define the adjusted error for each household as $\hat{E}_{(4)} := \left( \sum_{i=1}^{n} \hat{E}_4^\omega\right)^{\frac{1}{4}}$, where $n$ is the length of the forecast. As for the $4^{\text{th}}-$norm error, figure \ref{fig:Adj_err_all} shows the adjusted error for each household, $\hat{E}_{(4)}$, for each forecast in black with the flat blue line showing their average. These averages are given in table \ref{tab:errs}.

\begin{figure}
\centering
\includegraphics[scale=0.5]{Errors_Adj.pdf}
\caption{\label{fig:Adj_err_all} Adjusted error for each forecast of each household (black) and their average (blue).}
\end{figure}


\begin{table}
\centering
\begin{tabular}{|c|c|c|}
\hline
 & $E_4$ & $\hat{E}_{(4)}$ \\
 \hline
\textbf{SD} & 3.829 & 3.496 \\
\textbf{LW} & 4.76 & 4.100 \\ 
\textbf{AA} & 4.125 & \textbf{3.396} \\ 
\textbf{WA} & 3.866 & 3.542 \\ 
\textbf{BRR} & \textbf{3.805} & 3.575 \\
\hline
\end{tabular}
\caption{Mean of $4^{\text{th}}-$norm error and adjusted error with the bold showing the lowest error in each metric.}
\label{tab:errs}
\end{table}


Table \ref{tab:errs} shows the value of the blue lines in each of forecasts using both error metrics. There are many disagreements both qualitatively and quantitatively but we discuss the similarities first. In both cases, the LW forecast is the worst method. This intuitively makes sense since we know the LW forecast cannot take into account the week to week variability. While the WA forecast was generated using more data, due to the smoothing effect of regression, the WA forecast was judged to the worse than the SD forecast by both $E_4$ and $\hat{E}_4$. Notably though, the BRR forecast performed better by one measure and worse by the other. Nonetheless, regardless of which error measure is used and which of the forecasts is being considered, the error is  roughly between 3 and 4 kWh. This is quite large considering that the $75^{\text{th}}$ percentile is around the 0.5 kWh meaning that really none of these algorithms are able to mimic reality well enough and may need additional information such as temperature.

From table \ref{tab:errs}, it is clear that the choice of metric alters not only the error but also the conclusion regarding which algorithm is better. If the $4^{\text{th}}-$norm error metric is used, then the BRR forecast is the best and LW forecast is the worst with the AA forecast being judged the second worst but the same forecast is judged to be the best when the adjusted error measure is used. Based on the qualitative comparison in figures \ref{fig:SDforecast} - \ref{fig:BR_forecast_P1}, our understanding of forecasting electric load is better confirmed by the results from $\hat{E}_{(4)}$ and thus for the moment we will use the adjusted error measure to conclude which of the algorithms is better.

Thus far we have established that by both error measures the SD forecast performs quite well. Since it is a commonly used method we will use this as the benchmark. Regardless of which error measure we consider, we have made some improvements to the SD forecast. If we decide to use the the $E_4$, the improvement has been made by introducing bayesian framework along with regression techniques. If instead we use the $\hat{E}_{(4)}$, it is the local permutation in time that has allowed for improvements to be made. Again due to the prevalence and transparency of the $p^{\text{th}}-$norm error, we will use $E_4$ as the benchmark error measure but we remark that using the adjusted $4^{\text{th}}-$norm error is more appropriate for judging STLF at LV levels. 

Most applications in STLF are especially concerned with peaks however as we have seen, many of the common forecasting algorithms smooth profiles. Thus forecasting of peaks cannot be done only with existing algorithms. A new framework, one developed for the study of peaks, namely extreme value statistics, must be combined with what exists already. %Moreover, a forecast or a profile which (ideally) is peaky must be evaluated using error measures accordingly.




